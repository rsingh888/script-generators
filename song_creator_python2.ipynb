{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "song-creator-python2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1ea-E6Iixr1tyIAYuIrqZOtP4AsEl225G",
          "timestamp": 1528207374795
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kw0v1V5LnruF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b5358471-0978-41e9-d9f6-4d034bbccecd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528213600330,
          "user_tz": -60,
          "elapsed": 5485,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall -y PyDrive\n",
        "!pip uninstall -y torchvision\n",
        "!pip uninstall -y visualization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling PyDrive-1.3.1:\n",
            "  Successfully uninstalled PyDrive-1.3.1\n",
            "Uninstalling torchvision-0.2.1:\n",
            "  Successfully uninstalled torchvision-0.2.1\n",
            "Uninstalling visualization-0.0.1:\n",
            "  Successfully uninstalled visualization-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6WTlejKXDjqJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4ed68170-0933-4654-ec5b-9c1c63f6024f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528285195058,
          "user_tz": -60,
          "elapsed": 111612,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q torchvision\n",
        "!pip install -U -q visualization"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x563516524000 @  0x7ffae50af1c4 0x5634bb8270d8 0x5634bb910d5d 0x5634bb83a77a 0x5634bb83f462 0x5634bb837b3a 0x5634bb83f82e 0x5634bb837b3a 0x5634bb83f82e 0x5634bb837b3a 0x5634bb83f82e 0x5634bb837b3a 0x5634bb83fe1f 0x5634bb837b3a 0x5634bb83f82e 0x5634bb837b3a 0x5634bb83f82e 0x5634bb83f462 0x5634bb83f462 0x5634bb837b3a 0x5634bb83fe1f 0x5634bb83f462 0x5634bb837b3a 0x5634bb83fe1f 0x5634bb837b3a 0x5634bb83fe1f 0x5634bb837b3a 0x5634bb83f82e 0x5634bb837b3a 0x5634bb86850f 0x5634bb863202\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "88jZjWlGDnts",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Nrjbh4YDwQ6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V9XW0e_mENbw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "import sys, os\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "all_characters = string.printable\n",
        "number_of_characters = len(all_characters)\n",
        "\n",
        "\n",
        "def character_to_label(character):\n",
        "    \"\"\"Returns a one-hot-encoded tensor given a character.\n",
        "    \n",
        "    Uses string.printable as a dictionary.\n",
        "        \n",
        "    Parameters\n",
        "    ----------\n",
        "    character : str\n",
        "        A character\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    one_hot_tensor : Tensor of shape (1, number_of_characters)\n",
        "        One-hot-encoded tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    character_label = all_characters.find(character)\n",
        "        \n",
        "    return character_label\n",
        "\n",
        "def string_to_labels(character_string):\n",
        "    \n",
        "    return map(lambda character: character_to_label(character), character_string)\n",
        "\n",
        "def pad_sequence(seq, max_length, pad_label=100):\n",
        "    \n",
        "    seq += [pad_label for i in range(max_length - len(seq))]\n",
        "    \n",
        "    return seq\n",
        "\n",
        "\n",
        "class LyricsGenerationDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file_path, minimum_song_count=None, artists=None):\n",
        "        \n",
        "        downloaded = drive.CreateFile({'id':'1sSV6SCFPfl9I7jQ67mTGmdGVScytFC1p'}) # replace fileid with Id of file you want to access\n",
        "        downloaded.GetContentFile('poems.csv') # now you can use export.csv \n",
        "        \n",
        "        self.lyrics_dataframe = pd.read_csv(csv_file_path)\n",
        "        \n",
        "        if artists:\n",
        "            \n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.artist.isin(artists)]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        if minimum_song_count:\n",
        "        \n",
        "            # Getting artists that have 70+ songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('artist').filter(lambda x: len(x) > minimum_song_count)\n",
        "            # Reindex .loc after we fetched random songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_text_len = self.lyrics_dataframe.text.str.len().max()\n",
        "        \n",
        "        whole_dataset_len = len(self.lyrics_dataframe)\n",
        "        \n",
        "        self.indexes = range(whole_dataset_len)\n",
        "        \n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "        \n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.indexes)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        index = self.indexes[index]\n",
        "        \n",
        "        sequence_raw_string = self.lyrics_dataframe.loc[index].text\n",
        "        \n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        \n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "        \n",
        "        # Shifted by one char\n",
        "        input_string_labels = sequence_string_labels[:-1]\n",
        "        output_string_labels = sequence_string_labels[1:]\n",
        "                \n",
        "        # pad sequence so that all of them have the same lenght\n",
        "        # Otherwise the batching won't work\n",
        "        input_string_labels_padded = pad_sequence(input_string_labels, max_length=self.max_text_len)\n",
        "        \n",
        "        output_string_labels_padded = pad_sequence(output_string_labels, max_length=self.max_text_len, pad_label=-100)\n",
        "        \n",
        "        return (torch.LongTensor(input_string_labels_padded),\n",
        "                torch.LongTensor(output_string_labels_padded),\n",
        "                torch.LongTensor([sequence_length]) )\n",
        "\n",
        "    \n",
        "def post_process_sequence_batch(batch_tuple):\n",
        "    \n",
        "    input_sequences, output_sequences, lengths = batch_tuple\n",
        "    \n",
        "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "    splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                               splitted_output_sequence_batch,\n",
        "                               splitted_lengths_batch)\n",
        "\n",
        "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                         key=lambda p: int(p[2]),\n",
        "                                         reverse=True)\n",
        "\n",
        "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "\n",
        "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "\n",
        "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "    \n",
        "    # pytorch's api for rnns wants lenghts to be list of ints\n",
        "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
        "    \n",
        "    \n",
        "    return input_sequence_batch_transposed, output_sequence_batch_sorted, lengths_batch_sorted_list\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # Converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "        \n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        \n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "        \n",
        "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    \n",
        "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        embedded = self.encoder(input_sequences)\n",
        "\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_sequences_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        \n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        \n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden\n",
        "\n",
        "    \n",
        "trainset = LyricsGenerationDataset(csv_file_path='poems.csv')\n",
        "\n",
        "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=50,\n",
        "                                              shuffle=True, num_workers=0, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0FX-XsDE3dh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "rnn = RNN(input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters))\n",
        "rnn.cuda()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "def sample_from_rnn(starting_sting=\"Why\", sample_length=300, temperature=1):\n",
        "\n",
        "    sampled_string = starting_sting\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_sting) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = Variable(first_input)\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in xrange(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature) ).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "\n",
        "        current_input = Variable(predicted_label.unsqueeze(1))\n",
        "\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3sULXHuFHPW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "outputId": "07f71155-3fc9-4509-a028-169a92d52884",
        "executionInfo": {
          "status": "error",
          "timestamp": 1528204604677,
          "user_tz": -60,
          "elapsed": 593,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# importing a list that can be binded with a figure and update\n",
        "# the figure when being appended\n",
        "from visualization import VizList\n",
        "\n",
        "# Creating figure, axes and binding to lists \n",
        "f, (loss_axis, validation_axis, train_axis) = plt.subplots(3, 1)\n",
        "\n",
        "loss_axis.plot([], [])\n",
        "validation_axis.plot([], [])\n",
        "train_axis.plot([], [])\n",
        "\n",
        "\n",
        "loss_list = VizList()\n",
        "validation_list = VizList()\n",
        "train_list = VizList()\n",
        "\n",
        "loss_list.bind_to_axis(loss_axis)\n",
        "validation_list.bind_to_axis(validation_axis)\n",
        "train_list.bind_to_axis(train_axis)\n",
        "\n",
        "loss_axis.set_title('Training Loss')\n",
        "validation_axis.set_title('Validation Set Accuracy')\n",
        "train_axis.set_title('Training Set Accuracy')\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-38f1123d0477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# importing a list that can be binded with a figure and update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the figure when being appended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVizList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Creating figure, axes and binding to lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/visualization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvisualizer2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisualizer2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvisualizer3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisualizer3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/visualization/visualizer2d.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmtri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautolab_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mperception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColorImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDepthImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrayscaleImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRgbdImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGdImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSegmentationImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/autolab_core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdual_quaternion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDualQuaternion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTerminateException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexperiment_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperimentLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjson_serialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBagOfPoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBagOfVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlane3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/autolab_core/experiment_logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcsv_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCSVModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0myaml_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYamlConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experiment_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/autolab_core/yaml_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mruamel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named ruamel.yaml",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gqHf8h9qF8ki",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 7143
        },
        "outputId": "cfbcd3db-c33e-4dbf-fee0-88ae88ce837b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528299497505,
          "user_tz": -60,
          "elapsed": 13696548,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "clip = 1.0\n",
        "epochs_number = 200\n",
        "loss_list = list()\n",
        "\n",
        "print_every = 11\n",
        "steps = 0\n",
        "\n",
        "for epoch_number in range(epochs_number):\n",
        "  running_loss =0\n",
        "  print(len(trainset_loader))\n",
        "  for batch in trainset_loader:\n",
        "    steps +=1\n",
        "    post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "\n",
        "    input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "    output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
        "    input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
        "        \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
        "       \n",
        "    loss = criterion(logits, output_sequences_batch_var)\n",
        "    loss_list.append( loss.data[0] )\n",
        "    loss.backward()\n",
        "    running_loss +=loss.data[0]\n",
        "\n",
        "    #torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "    if steps%print_every == 0:\n",
        "      print(\"Epoch: {}/{}.. \".format(epoch_number+1, epochs_number),\n",
        "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every))           \n",
        "      running_loss = 0\n",
        "            \n",
        "print(sample_from_rnn(\"World\"))\n",
        "torch.save(rnn.state_dict(), 'conditional_rnn.pth')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Epoch: 1/200.. ', 'Training Loss: 3.510.. ')\n",
            "11\n",
            "('Epoch: 2/200.. ', 'Training Loss: 2.879.. ')\n",
            "11\n",
            "('Epoch: 3/200.. ', 'Training Loss: 2.530.. ')\n",
            "11\n",
            "('Epoch: 4/200.. ', 'Training Loss: 2.303.. ')\n",
            "11\n",
            "('Epoch: 5/200.. ', 'Training Loss: 2.178.. ')\n",
            "11\n",
            "('Epoch: 6/200.. ', 'Training Loss: 2.090.. ')\n",
            "11\n",
            "('Epoch: 7/200.. ', 'Training Loss: 2.022.. ')\n",
            "11\n",
            "('Epoch: 8/200.. ', 'Training Loss: 1.970.. ')\n",
            "11\n",
            "('Epoch: 9/200.. ', 'Training Loss: 1.922.. ')\n",
            "11\n",
            "('Epoch: 10/200.. ', 'Training Loss: 1.878.. ')\n",
            "11\n",
            "('Epoch: 11/200.. ', 'Training Loss: 1.847.. ')\n",
            "11\n",
            "('Epoch: 12/200.. ', 'Training Loss: 1.811.. ')\n",
            "11\n",
            "('Epoch: 13/200.. ', 'Training Loss: 1.783.. ')\n",
            "11\n",
            "('Epoch: 14/200.. ', 'Training Loss: 1.753.. ')\n",
            "11\n",
            "('Epoch: 15/200.. ', 'Training Loss: 1.733.. ')\n",
            "11\n",
            "('Epoch: 16/200.. ', 'Training Loss: 1.718.. ')\n",
            "11\n",
            "('Epoch: 17/200.. ', 'Training Loss: 1.697.. ')\n",
            "11\n",
            "('Epoch: 18/200.. ', 'Training Loss: 1.675.. ')\n",
            "11\n",
            "('Epoch: 19/200.. ', 'Training Loss: 1.657.. ')\n",
            "11\n",
            "('Epoch: 20/200.. ', 'Training Loss: 1.643.. ')\n",
            "11\n",
            "('Epoch: 21/200.. ', 'Training Loss: 1.626.. ')\n",
            "11\n",
            "('Epoch: 22/200.. ', 'Training Loss: 1.612.. ')\n",
            "11\n",
            "('Epoch: 23/200.. ', 'Training Loss: 1.602.. ')\n",
            "11\n",
            "('Epoch: 24/200.. ', 'Training Loss: 1.589.. ')\n",
            "11\n",
            "('Epoch: 25/200.. ', 'Training Loss: 1.572.. ')\n",
            "11\n",
            "('Epoch: 26/200.. ', 'Training Loss: 1.561.. ')\n",
            "11\n",
            "('Epoch: 27/200.. ', 'Training Loss: 1.546.. ')\n",
            "11\n",
            "('Epoch: 28/200.. ', 'Training Loss: 1.536.. ')\n",
            "11\n",
            "('Epoch: 29/200.. ', 'Training Loss: 1.527.. ')\n",
            "11\n",
            "('Epoch: 30/200.. ', 'Training Loss: 1.516.. ')\n",
            "11\n",
            "('Epoch: 31/200.. ', 'Training Loss: 1.508.. ')\n",
            "11\n",
            "('Epoch: 32/200.. ', 'Training Loss: 1.501.. ')\n",
            "11\n",
            "('Epoch: 33/200.. ', 'Training Loss: 1.489.. ')\n",
            "11\n",
            "('Epoch: 34/200.. ', 'Training Loss: 1.479.. ')\n",
            "11\n",
            "('Epoch: 35/200.. ', 'Training Loss: 1.469.. ')\n",
            "11\n",
            "('Epoch: 36/200.. ', 'Training Loss: 1.469.. ')\n",
            "11\n",
            "('Epoch: 37/200.. ', 'Training Loss: 1.456.. ')\n",
            "11\n",
            "('Epoch: 38/200.. ', 'Training Loss: 1.443.. ')\n",
            "11\n",
            "('Epoch: 39/200.. ', 'Training Loss: 1.441.. ')\n",
            "11\n",
            "('Epoch: 40/200.. ', 'Training Loss: 1.432.. ')\n",
            "11\n",
            "('Epoch: 41/200.. ', 'Training Loss: 1.418.. ')\n",
            "11\n",
            "('Epoch: 42/200.. ', 'Training Loss: 1.418.. ')\n",
            "11\n",
            "('Epoch: 43/200.. ', 'Training Loss: 1.407.. ')\n",
            "11\n",
            "('Epoch: 44/200.. ', 'Training Loss: 1.402.. ')\n",
            "11\n",
            "('Epoch: 45/200.. ', 'Training Loss: 1.399.. ')\n",
            "11\n",
            "('Epoch: 46/200.. ', 'Training Loss: 1.389.. ')\n",
            "11\n",
            "('Epoch: 47/200.. ', 'Training Loss: 1.385.. ')\n",
            "11\n",
            "('Epoch: 48/200.. ', 'Training Loss: 1.376.. ')\n",
            "11\n",
            "('Epoch: 49/200.. ', 'Training Loss: 1.374.. ')\n",
            "11\n",
            "('Epoch: 50/200.. ', 'Training Loss: 1.362.. ')\n",
            "11\n",
            "('Epoch: 51/200.. ', 'Training Loss: 1.358.. ')\n",
            "11\n",
            "('Epoch: 52/200.. ', 'Training Loss: 1.346.. ')\n",
            "11\n",
            "('Epoch: 53/200.. ', 'Training Loss: 1.338.. ')\n",
            "11\n",
            "('Epoch: 54/200.. ', 'Training Loss: 1.343.. ')\n",
            "11\n",
            "('Epoch: 55/200.. ', 'Training Loss: 1.334.. ')\n",
            "11\n",
            "('Epoch: 56/200.. ', 'Training Loss: 1.326.. ')\n",
            "11\n",
            "('Epoch: 57/200.. ', 'Training Loss: 1.316.. ')\n",
            "11\n",
            "('Epoch: 58/200.. ', 'Training Loss: 1.305.. ')\n",
            "11\n",
            "('Epoch: 59/200.. ', 'Training Loss: 1.302.. ')\n",
            "11\n",
            "('Epoch: 60/200.. ', 'Training Loss: 1.297.. ')\n",
            "11\n",
            "('Epoch: 61/200.. ', 'Training Loss: 1.291.. ')\n",
            "11\n",
            "('Epoch: 62/200.. ', 'Training Loss: 1.285.. ')\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "('Epoch: 63/200.. ', 'Training Loss: 1.279.. ')\n",
            "11\n",
            "('Epoch: 64/200.. ', 'Training Loss: 1.278.. ')\n",
            "11\n",
            "('Epoch: 65/200.. ', 'Training Loss: 1.270.. ')\n",
            "11\n",
            "('Epoch: 66/200.. ', 'Training Loss: 1.268.. ')\n",
            "11\n",
            "('Epoch: 67/200.. ', 'Training Loss: 1.259.. ')\n",
            "11\n",
            "('Epoch: 68/200.. ', 'Training Loss: 1.254.. ')\n",
            "11\n",
            "('Epoch: 69/200.. ', 'Training Loss: 1.239.. ')\n",
            "11\n",
            "('Epoch: 70/200.. ', 'Training Loss: 1.236.. ')\n",
            "11\n",
            "('Epoch: 71/200.. ', 'Training Loss: 1.226.. ')\n",
            "11\n",
            "('Epoch: 72/200.. ', 'Training Loss: 1.222.. ')\n",
            "11\n",
            "('Epoch: 73/200.. ', 'Training Loss: 1.214.. ')\n",
            "11\n",
            "('Epoch: 74/200.. ', 'Training Loss: 1.213.. ')\n",
            "11\n",
            "('Epoch: 75/200.. ', 'Training Loss: 1.206.. ')\n",
            "11\n",
            "('Epoch: 76/200.. ', 'Training Loss: 1.204.. ')\n",
            "11\n",
            "('Epoch: 77/200.. ', 'Training Loss: 1.198.. ')\n",
            "11\n",
            "('Epoch: 78/200.. ', 'Training Loss: 1.188.. ')\n",
            "11\n",
            "('Epoch: 79/200.. ', 'Training Loss: 1.178.. ')\n",
            "11\n",
            "('Epoch: 80/200.. ', 'Training Loss: 1.175.. ')\n",
            "11\n",
            "('Epoch: 81/200.. ', 'Training Loss: 1.170.. ')\n",
            "11\n",
            "('Epoch: 82/200.. ', 'Training Loss: 1.168.. ')\n",
            "11\n",
            "('Epoch: 83/200.. ', 'Training Loss: 1.161.. ')\n",
            "11\n",
            "('Epoch: 84/200.. ', 'Training Loss: 1.149.. ')\n",
            "11\n",
            "('Epoch: 85/200.. ', 'Training Loss: 1.145.. ')\n",
            "11\n",
            "('Epoch: 86/200.. ', 'Training Loss: 1.139.. ')\n",
            "11\n",
            "('Epoch: 87/200.. ', 'Training Loss: 1.134.. ')\n",
            "11\n",
            "('Epoch: 88/200.. ', 'Training Loss: 1.129.. ')\n",
            "11\n",
            "('Epoch: 89/200.. ', 'Training Loss: 1.119.. ')\n",
            "11\n",
            "('Epoch: 90/200.. ', 'Training Loss: 1.109.. ')\n",
            "11\n",
            "('Epoch: 91/200.. ', 'Training Loss: 1.105.. ')\n",
            "11\n",
            "('Epoch: 92/200.. ', 'Training Loss: 1.103.. ')\n",
            "11\n",
            "('Epoch: 93/200.. ', 'Training Loss: 1.095.. ')\n",
            "11\n",
            "('Epoch: 94/200.. ', 'Training Loss: 1.088.. ')\n",
            "11\n",
            "('Epoch: 95/200.. ', 'Training Loss: 1.085.. ')\n",
            "11\n",
            "('Epoch: 96/200.. ', 'Training Loss: 1.076.. ')\n",
            "11\n",
            "('Epoch: 97/200.. ', 'Training Loss: 1.073.. ')\n",
            "11\n",
            "('Epoch: 98/200.. ', 'Training Loss: 1.060.. ')\n",
            "11\n",
            "('Epoch: 99/200.. ', 'Training Loss: 1.055.. ')\n",
            "11\n",
            "('Epoch: 100/200.. ', 'Training Loss: 1.059.. ')\n",
            "11\n",
            "('Epoch: 101/200.. ', 'Training Loss: 1.041.. ')\n",
            "11\n",
            "('Epoch: 102/200.. ', 'Training Loss: 1.028.. ')\n",
            "11\n",
            "('Epoch: 103/200.. ', 'Training Loss: 1.023.. ')\n",
            "11\n",
            "('Epoch: 104/200.. ', 'Training Loss: 1.017.. ')\n",
            "11\n",
            "('Epoch: 105/200.. ', 'Training Loss: 1.012.. ')\n",
            "11\n",
            "('Epoch: 106/200.. ', 'Training Loss: 1.009.. ')\n",
            "11\n",
            "('Epoch: 107/200.. ', 'Training Loss: 1.008.. ')\n",
            "11\n",
            "('Epoch: 108/200.. ', 'Training Loss: 0.989.. ')\n",
            "11\n",
            "('Epoch: 109/200.. ', 'Training Loss: 0.977.. ')\n",
            "11\n",
            "('Epoch: 110/200.. ', 'Training Loss: 0.983.. ')\n",
            "11\n",
            "('Epoch: 111/200.. ', 'Training Loss: 0.976.. ')\n",
            "11\n",
            "('Epoch: 112/200.. ', 'Training Loss: 0.975.. ')\n",
            "11\n",
            "('Epoch: 113/200.. ', 'Training Loss: 0.965.. ')\n",
            "11\n",
            "('Epoch: 114/200.. ', 'Training Loss: 0.956.. ')\n",
            "11\n",
            "('Epoch: 115/200.. ', 'Training Loss: 0.946.. ')\n",
            "11\n",
            "('Epoch: 116/200.. ', 'Training Loss: 0.937.. ')\n",
            "11\n",
            "('Epoch: 117/200.. ', 'Training Loss: 0.934.. ')\n",
            "11\n",
            "('Epoch: 118/200.. ', 'Training Loss: 0.932.. ')\n",
            "11\n",
            "('Epoch: 119/200.. ', 'Training Loss: 0.920.. ')\n",
            "11\n",
            "('Epoch: 120/200.. ', 'Training Loss: 0.919.. ')\n",
            "11\n",
            "('Epoch: 121/200.. ', 'Training Loss: 0.907.. ')\n",
            "11\n",
            "('Epoch: 122/200.. ', 'Training Loss: 0.899.. ')\n",
            "11\n",
            "('Epoch: 123/200.. ', 'Training Loss: 0.890.. ')\n",
            "11\n",
            "('Epoch: 124/200.. ', 'Training Loss: 0.891.. ')\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "('Epoch: 125/200.. ', 'Training Loss: 0.890.. ')\n",
            "11\n",
            "('Epoch: 126/200.. ', 'Training Loss: 0.873.. ')\n",
            "11\n",
            "('Epoch: 127/200.. ', 'Training Loss: 0.863.. ')\n",
            "11\n",
            "('Epoch: 128/200.. ', 'Training Loss: 0.851.. ')\n",
            "11\n",
            "('Epoch: 129/200.. ', 'Training Loss: 0.845.. ')\n",
            "11\n",
            "('Epoch: 130/200.. ', 'Training Loss: 0.840.. ')\n",
            "11\n",
            "('Epoch: 131/200.. ', 'Training Loss: 0.829.. ')\n",
            "11\n",
            "('Epoch: 132/200.. ', 'Training Loss: 0.819.. ')\n",
            "11\n",
            "('Epoch: 133/200.. ', 'Training Loss: 0.825.. ')\n",
            "11\n",
            "('Epoch: 134/200.. ', 'Training Loss: 0.811.. ')\n",
            "11\n",
            "('Epoch: 135/200.. ', 'Training Loss: 0.809.. ')\n",
            "11\n",
            "('Epoch: 136/200.. ', 'Training Loss: 0.792.. ')\n",
            "11\n",
            "('Epoch: 137/200.. ', 'Training Loss: 0.791.. ')\n",
            "11\n",
            "('Epoch: 138/200.. ', 'Training Loss: 0.783.. ')\n",
            "11\n",
            "('Epoch: 139/200.. ', 'Training Loss: 0.779.. ')\n",
            "11\n",
            "('Epoch: 140/200.. ', 'Training Loss: 0.770.. ')\n",
            "11\n",
            "('Epoch: 141/200.. ', 'Training Loss: 0.782.. ')\n",
            "11\n",
            "('Epoch: 142/200.. ', 'Training Loss: 0.771.. ')\n",
            "11\n",
            "('Epoch: 143/200.. ', 'Training Loss: 0.740.. ')\n",
            "11\n",
            "('Epoch: 144/200.. ', 'Training Loss: 0.744.. ')\n",
            "11\n",
            "('Epoch: 145/200.. ', 'Training Loss: 0.736.. ')\n",
            "11\n",
            "('Epoch: 146/200.. ', 'Training Loss: 0.735.. ')\n",
            "11\n",
            "('Epoch: 147/200.. ', 'Training Loss: 0.728.. ')\n",
            "11\n",
            "('Epoch: 148/200.. ', 'Training Loss: 0.723.. ')\n",
            "11\n",
            "('Epoch: 149/200.. ', 'Training Loss: 0.715.. ')\n",
            "11\n",
            "('Epoch: 150/200.. ', 'Training Loss: 0.700.. ')\n",
            "11\n",
            "('Epoch: 151/200.. ', 'Training Loss: 0.690.. ')\n",
            "11\n",
            "('Epoch: 152/200.. ', 'Training Loss: 0.693.. ')\n",
            "11\n",
            "('Epoch: 153/200.. ', 'Training Loss: 0.678.. ')\n",
            "11\n",
            "('Epoch: 154/200.. ', 'Training Loss: 0.672.. ')\n",
            "11\n",
            "('Epoch: 155/200.. ', 'Training Loss: 0.674.. ')\n",
            "11\n",
            "('Epoch: 156/200.. ', 'Training Loss: 0.657.. ')\n",
            "11\n",
            "('Epoch: 157/200.. ', 'Training Loss: 0.643.. ')\n",
            "11\n",
            "('Epoch: 158/200.. ', 'Training Loss: 0.659.. ')\n",
            "11\n",
            "('Epoch: 159/200.. ', 'Training Loss: 0.663.. ')\n",
            "11\n",
            "('Epoch: 160/200.. ', 'Training Loss: 0.657.. ')\n",
            "11\n",
            "('Epoch: 161/200.. ', 'Training Loss: 0.643.. ')\n",
            "11\n",
            "('Epoch: 162/200.. ', 'Training Loss: 0.626.. ')\n",
            "11\n",
            "('Epoch: 163/200.. ', 'Training Loss: 0.622.. ')\n",
            "11\n",
            "('Epoch: 164/200.. ', 'Training Loss: 0.610.. ')\n",
            "11\n",
            "('Epoch: 165/200.. ', 'Training Loss: 0.600.. ')\n",
            "11\n",
            "('Epoch: 166/200.. ', 'Training Loss: 0.591.. ')\n",
            "11\n",
            "('Epoch: 167/200.. ', 'Training Loss: 0.584.. ')\n",
            "11\n",
            "('Epoch: 168/200.. ', 'Training Loss: 0.581.. ')\n",
            "11\n",
            "('Epoch: 169/200.. ', 'Training Loss: 0.569.. ')\n",
            "11\n",
            "('Epoch: 170/200.. ', 'Training Loss: 0.570.. ')\n",
            "11\n",
            "('Epoch: 171/200.. ', 'Training Loss: 0.569.. ')\n",
            "11\n",
            "('Epoch: 172/200.. ', 'Training Loss: 0.559.. ')\n",
            "11\n",
            "('Epoch: 173/200.. ', 'Training Loss: 0.549.. ')\n",
            "11\n",
            "('Epoch: 174/200.. ', 'Training Loss: 0.553.. ')\n",
            "11\n",
            "('Epoch: 175/200.. ', 'Training Loss: 0.537.. ')\n",
            "11\n",
            "('Epoch: 176/200.. ', 'Training Loss: 0.527.. ')\n",
            "11\n",
            "('Epoch: 177/200.. ', 'Training Loss: 0.532.. ')\n",
            "11\n",
            "('Epoch: 178/200.. ', 'Training Loss: 0.529.. ')\n",
            "11\n",
            "('Epoch: 179/200.. ', 'Training Loss: 0.532.. ')\n",
            "11\n",
            "('Epoch: 180/200.. ', 'Training Loss: 0.505.. ')\n",
            "11\n",
            "('Epoch: 181/200.. ', 'Training Loss: 0.514.. ')\n",
            "11\n",
            "('Epoch: 182/200.. ', 'Training Loss: 0.498.. ')\n",
            "11\n",
            "('Epoch: 183/200.. ', 'Training Loss: 0.500.. ')\n",
            "11\n",
            "('Epoch: 184/200.. ', 'Training Loss: 0.496.. ')\n",
            "11\n",
            "('Epoch: 185/200.. ', 'Training Loss: 0.493.. ')\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "('Epoch: 186/200.. ', 'Training Loss: 0.476.. ')\n",
            "11\n",
            "('Epoch: 187/200.. ', 'Training Loss: 0.476.. ')\n",
            "11\n",
            "('Epoch: 188/200.. ', 'Training Loss: 0.477.. ')\n",
            "11\n",
            "('Epoch: 189/200.. ', 'Training Loss: 0.464.. ')\n",
            "11\n",
            "('Epoch: 190/200.. ', 'Training Loss: 0.446.. ')\n",
            "11\n",
            "('Epoch: 191/200.. ', 'Training Loss: 0.439.. ')\n",
            "11\n",
            "('Epoch: 192/200.. ', 'Training Loss: 0.434.. ')\n",
            "11\n",
            "('Epoch: 193/200.. ', 'Training Loss: 0.432.. ')\n",
            "11\n",
            "('Epoch: 194/200.. ', 'Training Loss: 0.433.. ')\n",
            "11\n",
            "('Epoch: 195/200.. ', 'Training Loss: 0.435.. ')\n",
            "11\n",
            "('Epoch: 196/200.. ', 'Training Loss: 0.435.. ')\n",
            "11\n",
            "('Epoch: 197/200.. ', 'Training Loss: 0.417.. ')\n",
            "11\n",
            "('Epoch: 198/200.. ', 'Training Loss: 0.409.. ')\n",
            "11\n",
            "('Epoch: 199/200.. ', 'Training Loss: 0.406.. ')\n",
            "11\n",
            "('Epoch: 200/200.. ', 'Training Loss: 0.412.. ')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "World far\" from a hear refrinces with dully disgrace,\n",
            " \n",
            " Walked and fairer toops any jolinicor.\n",
            " \n",
            " We heard, then, not wine, and now, is preim all.\n",
            " \n",
            " \n",
            " \n",
            " They end my man who lighted breaves, repeats. Then,\n",
            " \n",
            " As think; the seene of loves shaft,\n",
            " \n",
            " I summony sprite, such first his queen, souls!\"\n",
            " \n",
            " The r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xO4n2PQtZaKd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c8497965-1562-44cd-ab21-cf4a35f74d91",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528300031604,
          "user_tz": -60,
          "elapsed": 776,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(sample_from_rnn(\"love\",196))\n",
        "# !df -h\n",
        "# !ps -ef | grep python\n",
        "\n",
        "# !kill -9 679"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "love I gone, to my love, who loves, and then\n",
            " \n",
            " From conquiands,\n",
            " \n",
            " My thoughts and graves the best,\n",
            " \n",
            " Drear there is no such to you;\n",
            " \n",
            " But Augusty, since to when I cry;\n",
            " \n",
            " Whet was his worke the pa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_7aUgewMnDiH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fcec8620-20c4-4f1e-f7c9-fda45ac725f0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528213385206,
          "user_tz": -60,
          "elapsed": 2050,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -lrt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 616\r\n",
            "drwxr-xr-x 1 root root   4096 Jun  5 15:34 datalab\r\n",
            "-rw-r--r-- 1 root root 625563 Jun  5 15:34 poems.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2W4qrzrKSopQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p84aMI1yS7Ng",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FbF2V5RVSQnY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "96c179a2-d78f-4b2f-be61-8687fa1e4332",
        "executionInfo": {
          "status": "error",
          "timestamp": 1528207947994,
          "user_tz": -60,
          "elapsed": 2204,
          "user": {
            "displayName": "Rajeev Singh",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103955521586798671934"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnt guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2a9b96e576ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPUtil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mGPUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGPUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# XXX: only one GPU on Colab and isnt guaranteed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/GPUtil/GPUtil.pyc\u001b[0m in \u001b[0;36mgetGPUs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetGPUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Get ID, processing and memory utilization for all GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nvidia-smi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"--query-gpu=index,uuid,utilization.gpu,memory.total,memory.used,memory.free,driver_version,name,gpu_serial,display_active,display_mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--format=csv,noheader,nounits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# output = output[2:-1] # Remove b' and ' from string added by python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mp2cread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2cwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 errread, errwrite)\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# Preserve original exception in case os.close raises.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, cwd, env, universal_newlines, startupinfo, creationflags, shell, to_close, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0mchild_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory"
          ]
        }
      ]
    }
  ]
}